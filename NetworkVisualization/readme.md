
##  Neural Networks Visualization

## Saliency Maps

A saliency map tells the degree to which each pixel in the image affects the classification score for that image.

<img src="https://github.com/shushukurov/ML_Portfolio/blob/main/NetworkVisualization/SilencyMaps.jpg" width=691 height=404>

## Adversarial Attacks

Use of image gradients to generate "adversarial attacks". Given an image and a target class, It is possible to perform gradient ascent over the image to maximize the target class, stopping when the network classifies the image as the target class.

<img src="https://github.com/shushukurov/ML_Portfolio/blob/main/NetworkVisualization/AdversarialAttack.jpeg" width=500 height=404>

## Class visualization

By starting with a random noise image and performing gradient ascent on a target class, It is possible to generate an image that the network will recognize as the target class.

<img src="https://github.com/shushukurov/ML_Portfolio/blob/main/NetworkVisualization/Class_Visual.png" width=404 height=404>
