# Will be uploaded soon

This is my implementation of Convolutional Neural Network, Batch Normalization and Kaiming initialization

In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery

![Alt Text](https://github.com/shushukurov/ML_Portfolio/blob/main/CNN_BatchNorm_Kaiming/CIFAR10_CNN.gif)

Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015.

![Alt Text](https://github.com/shushukurov/ML_Portfolio/blob/main/CNN_BatchNorm_Kaiming/BN.png)

Kaiming Initialization, or He Initialization, is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as ReLU activations.

![Alt Text](https://github.com/shushukurov/ML_Portfolio/blob/main/CNN_BatchNorm_Kaiming/KaimRelyXavier.png)
