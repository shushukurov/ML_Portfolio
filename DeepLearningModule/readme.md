## 3. DeepLearning

This Project consists of Modular implementation of Fully-Connected Neural Networks, Dropout and different optimizers (SGD, Momentum, Rmsprop, Adam)

## Modular implementation of Fully-Connected Neural Networks
Andrej Karphathy once wrote that ML engineers should have deep understanding of backpropagation. Therefore I implemented all necessary modules for Neural Networks from scratch using PyTorch GPU acceleration in order Improve knowledge of NN and Backprop.

<img src="https://github.com/shushukurov/ML_Portfolio/blob/main/DeepLearningModule/Backprop.png" width=800 height=400>

## Dropout

Dropout is a technique for regularizing neural networks by randomly setting some output activations to zero during the forward pass.

<img src="https://github.com/shushukurov/ML_Portfolio/blob/main/DeepLearningModule/Dropout.png" width=600 height=400>

## Optimizers (SGD, Momentum, Rmsprop, Adam)

So far I have used vanilla stochastic gradient descent (SGD) as my update rule. More sophisticated update rules can make it easier to train deep networks. Therefore I have implement a few of the most commonly used update rules (SGD, Momentum, RMsprop, Adam) and compare them to vanilla SGD.

![Alt Text](https://github.com/shushukurov/ML_Portfolio/blob/main/DeepLearningModule/Optimizers.png)

### Motivation
Andrej Karphathy once wrote that ML engineers should have deep understanding of backpropagation. 
Therefore I implemented all necessary modules for Neural Networks from scratch using PyTorch GPU acceleration 
in order Improve knowledge of NN and Backprop. Also I implemented and illustrated DropOut as a regularization technique for deep learning
