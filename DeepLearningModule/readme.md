This notebook consists of Modular implementation of Fully-Connected Neural Networks, Dropout and different optimizers (SGD, Momentum, Rmsprop, Adam)

Andrej Karphathy once wrote that ML engineers should have deep understanding of backpropagation. 
Therefore I implemented all necessary modules for Neural Networks from scratch using PyTorch GPU acceleration 
in order Improve knowledge of NN and Backprop. Also I implemented and illustrated DropOut as a regularization technique for deep learning
